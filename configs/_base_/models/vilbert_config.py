# model settings
model = dict(
    type='VilBERT',
    params=dict(
        v_feature_size=2048,
        v_hidden_size=1024,
        v_biattention_id=[0, 1, 2, 3, 4, 5],
        t_biattention_id=[6, 7, 8, 9, 10, 11],
        in_batch_pairs=False,
        fixed_t_layer=0,
        fixed_v_layer=0,
        fast_mode=False,
        with_coattention=True,
        v_num_attention_heads=8,
        dynamic_attention=False,
        visualization=False,
        v_attention_probs_dropout_prob=0.1,
        v_hidden_dropout_prob=0.1,
        v_intermediate_size=1024,
        v_hidden_act='gelu',
        v_num_hidden_layers=6,
        bi_hidden_size=1024,
        bi_num_attention_heads=8,
        # bi_intermediate_size=1024,
        # output_attentions=False,
        # output_hidden_states=False,
        # pooler_strategy='default',
        bert_model_name='bert-base-uncased',
        task_specific_tokens=False,
        # visual_embedding_dim=2048,
        # embedding_strategy='plain',
        # bypass_transformer=False,
        num_labels=3129,
        training_head_type='hateful_memes',
        fusion_method='mul',
        # special_visual_initialize=False,
        freeze_base=False,
        random_initialize=False))

loss = dict(type='BinaryCrossEntropyWithLogits')
