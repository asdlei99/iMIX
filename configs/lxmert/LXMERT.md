# LXMERT

## Introduction

```
@inproceedings{hu2019language,
  title={LXMERT: Learning Cross-Modality Encoder Representations
from Transformers},
  author={Hao Tan, Mohit Bansal},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10294--10303},
  year={2019}
}
```

## Results and Models

| Task |  Style  |    Accuracy    |                            Config                            |                           Download                           |
| :--: | :-----: | :------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| NLVR | pytorch | 74.82%(74.78%) | [config](https://mega.nz/file/WCpyGaqJ#akkYswi2LrRJk-EApR9G6lofQqGyKH24ara2oEyyPOU) | [model](https://mega.nz/file/CbowQCBK#mGZVUSVIggDtZ5pmI_Pg6CXfYTS626U0YzkqeSAfyKk) &#124; [log](https://mega.nz/file/2DhEVQQJ#oZO89gYobjKexnB1AisfCv06tzpw3UIEIgAcgZK5_tA) |
| GQA  | pytorch | 59.83%(59.87%) | [config](https://mega.nz/file/ye4ElQDA#8gqxPWdtBvLabJP_u2J7gZf6_W38o-a9-4mLpDqhTPc) | [model](https://mega.nz/file/7ewiECwA#Jb88_dtwac6a2p37DBeSoXD0kKXnmbXUsG0oEECk0ps) &#124; [log](https://mega.nz/file/eTgGRK6K#aMEvjSR3SxXE7SyttF_ANuhoG6Q8LNPA5z5fvmB9IV4) |
| VQA  | pytorch | 70.20%(70.09%) | [config](https://mega.nz/file/DPoUUIpb#g11gzZ0dfFG2e1KddvI6Q5axirtgy06jCpYTAXMLAZo) | [model](https://mega.nz/file/OW5GEIxb#TeXyG2OhV8ZoQ2ESGZOyhONlK0B9p0qwG4bBSkyIX0c) &#124; [log](https://mega.nz/file/6Cog0YDC#TS3Btj4_COSjzLV1v6t3Px0KsOSHZyO27eDLNrNXMzk) |

**Notes:**

- The accuracy values in the brackets represent those reported in the source code of https://github.com/airsplay/lxmert.
