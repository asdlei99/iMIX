{
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522,
  "bi_hidden_size":1024,
  "bi_num_attention_heads":8,
  "bi_intermediate_size": 1024,
  "bi_attention_type":1,
  "t_biattention_id":[6, 7, 8, 9, 10, 11],
  "pooling_method": "mul",
  "visual_target":0,
  "fast_mode":false,
  "fixed_v_layer":0,
  "fixed_t_layer":0,
  "in_batch_pairs":false,
  "fusion_method":"mul",
  "dynamic_attention":false,
  "with_coattention":true,
  "objective":0,
  "num_negative":128,
  "model":"bert",
  "task_specific_tokens":false,
  "visualization":false,
  "layer_norm_eps":1e-12,
  "v_config":{
    "feature_size": 2048,
    "target_size": 1601,
    "hidden_size": 1024,
    "num_hidden_layers":6,
    "num_attention_heads":8,
    "intermediate_size":1024,
    "attention_probs_dropout_prob":0.1,
    "hidden_act":"gelu",
    "hidden_dropout_prob":0.1,
    "initializer_range":0.02,
    "biattention_id":[0, 1, 2, 3, 4, 5]
  }
}
